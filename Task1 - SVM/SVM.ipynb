{"cells":[{"cell_type":"markdown","metadata":{"id":"1PKc2zVMek8Z"},"source":["#Classifying contextualized embeddings of the verbs"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1651421624700,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"f7KjHS1WQzN2"},"outputs":[],"source":["import json\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"XxKrrwqxEocJ"},"source":["Get the data from the TPAS corpus"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":235,"status":"ok","timestamp":1651422888993,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"1deCbB5LQdYS"},"outputs":[],"source":["def AccogliereCONT(filename):\n","    f = open(filename)\n","    r = json.load(f)\n","    data = r['data'] \n","    sent_lab= []\n","    sent_lab_tok = []\n","    for x in data: \n","        if x[0] == \"accogliere\":\n","            if x[1] == \"1\":\n","                tup = (0, x[2])\n","            if x[1] == \"2\":\n","                tup = (1, x[2])\n","            sent_lab.append(tup)\n","    for x in sent_lab:\n","        sen = x[1]\n","        sent_tokenized = sen.split(\" \")\n","        for tok in sent_tokenized:\n","            if \"accog\" in tok or \"accol\" in tok or \"ACCOL\" in tok or \"ACCOG\" in tok:\n","                trip = (x[0], tok, x[1])\n","                sent_lab_tok.append(trip)\n","                break\n","    sent_lab_tok = list(set(sent_lab_tok))\n","    sentences = []\n","    labels = []\n","    tokens = []\n","    for x in sent_lab_tok:\n","        labels.append(x[0])\n","        sentences.append(x[1])\n","        tokens.append(x[2])    \n","    D = {'labels': labels, 'tokens': tokens, 'sentences': sentences}\n","    #df = pd.DataFrame(D)\n","    return sent_lab_tok, D"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":489,"status":"ok","timestamp":1651422909795,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"--Mj-vjY3Iiv"},"outputs":[],"source":["def Ritirare(filename):\n","    f = open(filename)\n","    r = json.load(f)\n","    data = r['data'] \n","    sent_lab= []\n","    sent_lab_tok = []\n","    for x in data: \n","        if x[0] == \"ritirare\":\n","            if x[1] == \"1\":\n","                tup = (0, x[2])\n","            if x[1] == \"3\":\n","                tup = (1, x[2])\n","            if x[1] == \"5\":\n","              tup = (2, x[2])\n","            sent_lab.append(tup)\n","    for x in sent_lab:\n","        sen = x[1]\n","        sent_tokenized = sen.split(\" \")\n","        for tok in sent_tokenized:\n","            if \"ritir\" in tok:\n","                trip = (x[0], tok, x[1])\n","                sent_lab_tok.append(trip)\n","                break\n","    sent_lab_tok = list(set(sent_lab_tok))\n","    sentences = []\n","    labels = []\n","    tokens = []\n","    for x in sent_lab_tok:\n","        labels.append(x[0])\n","        sentences.append(x[1])\n","        tokens.append(x[2])    \n","    D = {'labels': labels, 'tokens': tokens, 'sentences': sentences}\n","    #df = pd.DataFrame(D)\n","    return sent_lab_tok, D"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":195,"status":"ok","timestamp":1651422914579,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"Mdw85kA-4U1W"},"outputs":[],"source":["def ACCUSARE(filename):\n","    f = open(filename)\n","    r = json.load(f)\n","    data = r['data'] \n","    sent_lab= []\n","    sent_lab_tok = []\n","    for x in data: \n","        if x[0] == \"accusare\":\n","            if x[1] == \"1\":\n","                tup = (0, x[2])\n","            if x[1] == \"2\":\n","                tup = (1, x[2])\n","            if x[1] == \"5\":\n","              tup = (2, x[2])\n","            if x[1] == \"6\":\n","              tup = (3, x[2])\n","            sent_lab.append(tup)\n","    for x in sent_lab:\n","        sen = x[1]\n","        sent_tokenized = sen.split(\" \")\n","        for tok in sent_tokenized:\n","            if \"accus\" in tok:\n","                trip = (x[0], tok, x[1])\n","                sent_lab_tok.append(trip)\n","                break\n","    sent_lab_tok = list(set(sent_lab_tok))\n","    sentences = []\n","    labels = []\n","    tokens = []\n","    for x in sent_lab_tok:\n","        labels.append(x[0])\n","        sentences.append(x[1])\n","        tokens.append(x[2])    \n","    D = {'labels': labels, 'tokens': tokens, 'sentences': sentences}\n","    #df = pd.DataFrame(D)\n","    return sent_lab_tok, D"]},{"cell_type":"markdown","metadata":{"id":"-k7xmIxZEtbO"},"source":["Get contextualized embeddings of the verb (as in https://discuss.huggingface.co/t/generate-raw-word-embeddings-using-transformer-models-like-bert-for-downstream-process/2958) "]},{"cell_type":"code","execution_count":4,"metadata":{"id":"GP3mq-bZA_vp"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in c:\\users\\marta\\anaconda3\\lib\\site-packages (4.18.0)\n","Requirement already satisfied: requests in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n","Requirement already satisfied: numpy>=1.17 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n","Requirement already satisfied: filelock in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (0.5.1)\n","Requirement already satisfied: sacremoses in c:\\users\\marta\\anaconda3\\lib\\site-packages (from transformers) (0.0.49)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n","Requirement already satisfied: idna<3,>=2.5 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\marta\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in c:\\users\\marta\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in c:\\users\\marta\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in c:\\users\\marta\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install transformers"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":10635,"status":"ok","timestamp":1651422947269,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"l0tqDcCwerAB"},"outputs":[],"source":[" import numpy as np\n"," import torch\n"," from transformers import AutoTokenizer, AutoModel\n"," \n"," def get_word_idx(sent: str, word: str):\n","     return sent.split(\" \").index(word)\n","\n"," def get_hidden_states(encoded, token_ids_word, model, layers):\n","     \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n","        Select only those subword token outputs that belong to our word of interest\n","        and average them.\"\"\"\n","     with torch.no_grad():\n","         output = model(**encoded)\n"," \n","     # Get all hidden states\n","     states = output.hidden_states\n","     # Stack and sum all requested layers\n","     output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n","     # Only select the tokens that constitute the requested word\n","     word_tokens_output = output[token_ids_word]\n"," \n","     return word_tokens_output.mean(dim=0)\n"," \n"," def get_word_vector(sent, idx, tokenizer, model, layers):\n","     \"\"\"Get a word vector by first tokenizing the input sentence, getting all token idxs\n","        that make up the word of interest, and then `get_hidden_states`.\"\"\"\n","     encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n","     # get all token idxs that belong to the word of interest\n","     token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n"," \n","     return get_hidden_states(encoded, token_ids_word, model, layers)\n"," \n"," def main(sent, tok, layers=None):\n","     # Use last four layers by default\n","     layers = [-4, -3, -2, -1] if layers is None else layers\n","\n","     tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\")\n","     model = AutoModel.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\", output_hidden_states=True) \n","     idx = get_word_idx(sent, tok)\n","\n","     word_embedding = get_word_vector(sent, idx, tokenizer, model, layers)\n","     return word_embedding \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USw8OMc7BdI2"},"outputs":[],"source":["if __name__ == '__main__':\n","\n","  Ls, D = ACCUSARE(\"G:\\My Drive\\TESI codici\\TPAS corpus.json\")\n","  vectors = []\n","\n","  for x in Ls:\n","    vec = main(x[2], x[1])\n","    vectors.append(vec)\n","  #prova_RITIRARE = main('ritira','Luca ritira i suoi giocattoli nella borsa verde')."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2140,"status":"ok","timestamp":1651424714083,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"lRJu43NBKqgg","outputId":"f7b0bf76-3c23-47c4-b76f-2d187403d9d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["     labels                                             tokens    sentences  \\\n","0         3  Al termine dell' intervento un Vigile del Fuoc...     accusato   \n","1         3  Molti bambini sottoposti a questo stress , se ...     accusare   \n","2         3  = = = = = = = = = = = = = = = = = = = = = = = ...     accusare   \n","3         3  Drammatico l' arrivo degli extracomunitari sul...     accusato   \n","4         3  Di successo in successo , che gli diedero fama...     accusare   \n","..      ...                                                ...          ...   \n","378       2  Porremo quindi come obiettivo della risorsa un...     accusare   \n","379       3  \" Durante la festa ha accusato un malessere , ...     accusato   \n","380       1  Se si vorrà assegnare una punizione particolar...     accusare   \n","381       3  Di conseguenza accuseranno i sintomi tipici de...  accuseranno   \n","382       1  È accusato di lesioni gravissime e detenzione ...     accusato   \n","\n","                   1                2                3                4  \\\n","0    tensor(-1.2685)  tensor(-0.7632)   tensor(0.9824)  tensor(-0.1418)   \n","1    tensor(-2.6198)  tensor(-3.1965)  tensor(-0.3040)  tensor(-0.5160)   \n","2    tensor(-0.7157)  tensor(-3.6796)  tensor(-0.1065)  tensor(-2.0043)   \n","3    tensor(-2.8278)   tensor(0.9215)   tensor(0.9334)  tensor(-0.3714)   \n","4    tensor(-0.4915)  tensor(-2.3411)   tensor(0.8903)  tensor(-0.9411)   \n","..               ...              ...              ...              ...   \n","378  tensor(-5.2824)  tensor(-2.0287)   tensor(1.8571)   tensor(1.1786)   \n","379  tensor(-1.0967)   tensor(0.1616)   tensor(1.1103)  tensor(-1.8320)   \n","380   tensor(2.1227)  tensor(-0.2461)   tensor(0.7849)  tensor(-1.4801)   \n","381   tensor(0.4020)   tensor(0.4789)  tensor(-0.7385)  tensor(-1.1292)   \n","382  tensor(-3.0901)  tensor(-0.7248)  tensor(-1.7372)  tensor(-2.9357)   \n","\n","                   5                6                7  ...              759  \\\n","0     tensor(0.7596)  tensor(-1.1628)   tensor(0.8328)  ...  tensor(-3.2856)   \n","1     tensor(0.5584)  tensor(-2.4746)  tensor(-0.3786)  ...   tensor(0.2411)   \n","2     tensor(1.4507)  tensor(-0.4563)  tensor(-2.2180)  ...  tensor(-0.0403)   \n","3     tensor(0.4222)  tensor(-4.6187)  tensor(-0.7913)  ...  tensor(-0.5716)   \n","4     tensor(1.0311)   tensor(0.6672)  tensor(-0.3088)  ...  tensor(-2.6490)   \n","..               ...              ...              ...  ...              ...   \n","378  tensor(-0.4432)  tensor(-0.3293)  tensor(-1.3765)  ...   tensor(1.5889)   \n","379   tensor(0.6387)   tensor(3.3676)  tensor(-1.4533)  ...  tensor(-0.1972)   \n","380   tensor(1.3670)   tensor(0.1945)   tensor(0.2788)  ...  tensor(-2.9087)   \n","381   tensor(0.4116)   tensor(0.0434)   tensor(0.0244)  ...   tensor(0.4230)   \n","382   tensor(2.2936)   tensor(0.9957)  tensor(-0.1517)  ...  tensor(-0.5310)   \n","\n","                 760              761              762              763  \\\n","0    tensor(-0.2292)   tensor(0.1021)   tensor(1.5993)  tensor(-0.0830)   \n","1    tensor(-3.8500)   tensor(1.0502)  tensor(-0.4900)  tensor(-1.4021)   \n","2    tensor(-2.4543)   tensor(0.4463)   tensor(2.4380)   tensor(0.5361)   \n","3    tensor(-1.3986)  tensor(-2.1545)  tensor(-1.9111)  tensor(-1.3875)   \n","4    tensor(-2.4931)  tensor(-0.9529)   tensor(0.6679)   tensor(1.4536)   \n","..               ...              ...              ...              ...   \n","378   tensor(1.1054)   tensor(1.6924)   tensor(4.3853)   tensor(0.7938)   \n","379  tensor(-2.2306)  tensor(-1.4391)   tensor(1.5935)  tensor(-1.5449)   \n","380   tensor(0.8399)  tensor(-1.3347)  tensor(-0.6035)  tensor(-1.4753)   \n","381  tensor(-0.4278)   tensor(2.3948)   tensor(0.0548)  tensor(-1.7480)   \n","382   tensor(1.5356)   tensor(0.4762)  tensor(-0.1543)   tensor(3.1757)   \n","\n","                 764              765              766              767  \\\n","0    tensor(-0.8142)   tensor(0.7526)   tensor(0.5199)  tensor(-1.1846)   \n","1     tensor(1.2908)  tensor(-3.0206)  tensor(-0.9427)  tensor(-1.0737)   \n","2    tensor(-1.2654)   tensor(0.1900)  tensor(-0.5175)  tensor(-1.7627)   \n","3    tensor(-2.7006)  tensor(-2.1643)  tensor(-1.4435)  tensor(-0.5819)   \n","4     tensor(0.5487)   tensor(1.0938)   tensor(0.7423)  tensor(-0.7461)   \n","..               ...              ...              ...              ...   \n","378   tensor(3.2458)   tensor(4.8825)  tensor(-0.4722)   tensor(0.1554)   \n","379   tensor(2.1022)   tensor(0.6380)   tensor(0.7382)   tensor(0.6833)   \n","380   tensor(1.2851)  tensor(-0.7999)  tensor(-0.2574)   tensor(1.0250)   \n","381   tensor(2.4018)  tensor(-2.5197)   tensor(0.4477)  tensor(-0.5253)   \n","382   tensor(0.0738)  tensor(-0.6341)   tensor(4.2213)  tensor(-0.7837)   \n","\n","                 768  \n","0     tensor(0.8441)  \n","1    tensor(-0.1573)  \n","2    tensor(-1.3239)  \n","3     tensor(0.8195)  \n","4     tensor(0.7996)  \n","..               ...  \n","378  tensor(-0.6841)  \n","379  tensor(-0.6055)  \n","380   tensor(1.3152)  \n","381  tensor(-1.4709)  \n","382  tensor(-4.6335)  \n","\n","[383 rows x 771 columns]\n"]}],"source":["  for x in range(768):\n","      A = []\n","      for S in vectors:\n","        A.append(S[x])\n","      D[str(x+1)] = A\n","      A=[]\n","\n","  df = pd.DataFrame(D)\n","  print(df)"]},{"cell_type":"markdown","metadata":{"id":"oCACmW4eGLsL"},"source":["Training the model using SVM"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":622,"status":"ok","timestamp":1651424720472,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"m6O_-Y_mGKbr"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":244,"status":"ok","timestamp":1651424723233,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"XnKTWEOuGTcs"},"outputs":[],"source":["X = df.drop(columns = ['sentences','labels', 'tokens'])\n","y = df.labels"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1651424725748,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"iFCAd67ZNyps","outputId":"00a0fde4-c34d-434e-a259-3095ec18a088"},"outputs":[{"data":{"text/plain":["383"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","len(X)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":619,"status":"ok","timestamp":1651424895520,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"E-W0SgikN9jy","outputId":"90f70af1-aa4b-451f-d9dd-09ec5618473f"},"outputs":[{"data":{"text/plain":["SVC()"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.svm import SVC\n","from sklearn.metrics import recall_score\n","model = SVC()\n","model.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1651424820050,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"2iH2YLKpOZsZ","outputId":"e6f2597c-a938-40bb-cb4b-0ffa8c70cbf5"},"outputs":[{"ename":"AttributeError","evalue":"'SVC' object has no attribute 'recall_score'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32m<ipython-input-15-3a0b80d5320f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'recall_score'"]}],"source":["model.recall_score(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"0Iu7vhwJQLnr"},"source":["The classifier have an accuracy around 85%"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":193,"status":"ok","timestamp":1651424821798,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"ZZL1zK1AzLe6","outputId":"40a9d2bd-5737-4846-efd7-e792792c002a"},"outputs":[{"data":{"text/plain":["array([3, 3, 0, 3, 0, 3, 3, 3, 0, 3, 3, 3, 1, 0, 3, 3, 3, 3, 3, 2, 3, 3,\n","       0, 3, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 0, 3, 0, 3, 3, 3, 3, 3, 2, 3,\n","       3, 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 0, 3, 3, 3, 3, 1, 3, 3, 3,\n","       0, 3, 3, 3, 0, 3, 3, 3, 3, 3, 3])"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model.predict(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"elapsed":208,"status":"error","timestamp":1650364181093,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"TKT_C5IWOade","outputId":"a9c5634a-c60f-4f84-d22b-cb537026c1a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n","  \"X does not have valid feature names, but\"\n","/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:746: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n","  array = np.asarray(array, order=order, dtype=dtype)\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-b683629b124a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#prova l'ho definito sopra, è una frase che ha label 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprova_RITIRARE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \"\"\"\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    596\u001b[0m                 \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m                 \u001b[0maccept_large_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m                 \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m             )\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"unsafe\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 746\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    747\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 raise ValueError(\n","\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"]}],"source":["#now test the model on new instances! \n","\n","sent_ACCOGLIERE = ['Quanto al danno morale , la domanda dovrebbe invece essere accolta solo sulla basa dell accertamento , in astratto , della responsabilità del sig. R per il reato di cui all art. 388 CP',\n","             'Rilevato che il T.A.R./Sicilia - sezione I di Catania , con ordinanza n. 12/2000 del 15 gennaio 2000 , ha accolto la sospensiva di detto decreto su ricorso giurisdizionale proposto da quattro consiglieri e che con successive ordinanze nn ',\n","             'Fino ad ora abbiamo accolto ben 4 coppie di fratellini e sorelline e altri 5 bimbi , in età compresa tra 1 anno e 10 anni .',\n","             \"La famiglia ha accolto il bambino come se fosse loro figlio, con amore e affetto\"]\n","\n","#prova l'ho definito sopra, è una frase che ha label 2\n","\n","model.predict(prova_RITIRARE)\n"]},{"cell_type":"markdown","metadata":{"id":"MTIYBFMSNtH7"},"source":["#Classifying sentence embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"1nHKzmqWPbs9"},"source":["Here I use the sentence_transformers library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9uB7QXhxnBuP"},"outputs":[],"source":["pip install -U sentence-transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGFf7BApTxgm"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5qgfpnUmydP"},"outputs":[],"source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n","\n","def ritornaembedding(sentence):\n","  embeddings = model.encode(sentence)\n","  return(embeddings)\n","\n","# sentence può essere o una frase o una lista di frasi\n","# a seconda ritorna una matrice o un vettore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aik4o4uDVWyA"},"outputs":[],"source":["# esempio\n","\n","sentences = ['Quanto al danno morale , la domanda dovrebbe invece essere accolta solo sulla basa dell accertamento , in astratto , della responsabilità del sig. R per il reato di cui all art. 388 CP',\n","             'Rilevato che il T.A.R./Sicilia - sezione I di Catania , con ordinanza n. 12/2000 del 15 gennaio 2000 , ha accolto la sospensiva di detto decreto su ricorso giurisdizionale proposto da quattro consiglieri e che con successive ordinanze nn ',\n","             'Fino ad ora abbiamo accolto ben 4 coppie di fratellini e sorelline e altri 5 bimbi , in età compresa tra 1 anno e 10 anni .',\n","             \"La famiglia ha accolto il bambino come se fosse loro figlio, con amore e affetto\"]\n","\n","prova = ritornaembedding(sentences)\n","len(prova)"]},{"cell_type":"markdown","metadata":{"id":"YIKL3cAonknY"},"source":["#Create a Pandas DataFrame for my dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXiyD6w8uUyf"},"outputs":[],"source":["import json\n","import pandas as pd"]},{"cell_type":"markdown","metadata":{"id":"Z19_zGdfn2gI"},"source":["La tabella dei sentence embeddings è\n","\n","```\n","sentence | vector | label_ind | label_name\n","```\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq_rZw8snrj1"},"outputs":[],"source":["def Accogliere(filename):\n","    f = open(filename)\n","    r = json.load(f)\n","    data = r['data'] \n","    labels = []\n","    label_names = ['']\n","    sentences = []\n","\n","    for x in data: \n","        if x[0] == \"accogliere\":\n","            if x[1] == \"1\":\n","                labels.append(0)\n","                sentences.append(x[2])\n","            if x[1] == \"2\":\n","                labels.append(1) \n","                sentences.append(x[2])\n","    embeddings = model.encode(sentences)\n","    print(type(embeddings))\n","    D = {\"sentences\": sentences, \"labels\": labels}\n","    for x in range(384):\n","      A = []\n","      for S in embeddings:\n","        A.append(S[x])\n","      D[str(x+1)] = A\n","      A=[]\n","\n","    return D "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Em1apX7honz9"},"outputs":[],"source":["Ds = Accogliere('/content/TPAS corpus.json')\n","df = pd.DataFrame(Ds)\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"YYw1dVdRCL64"},"source":["#Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LDtnh4LCkbT"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMPbabCrCnc7"},"outputs":[],"source":["X = df.drop(['sentences','labels'], axis='columns')\n","y = df.labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk-8ST0ZC4dz"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","len(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HWNaI1v2C-HC"},"outputs":[],"source":["from sklearn.svm import SVC\n","model = SVC()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1649596719357,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"BHQmsmiLDRhd","outputId":"11d84dc3-df06-4f9e-df57-6e05f92bd4f9"},"outputs":[{"data":{"text/plain":["SVC()"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["model.fit(X_train.values, y_train.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1649596722030,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"3X91oE4ZDIGF","outputId":"a13b1b03-71c7-457d-b00f-50c6215737a1"},"outputs":[{"data":{"text/plain":["0.6190476190476191"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["model.score(X_test.values, y_test.values)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1649596724543,"user":{"displayName":"MARTA RICCHIARDI","userId":"03986345094947965389"},"user_tz":-120},"id":"v_s3OCd3WB0S","outputId":"b77385aa-ff16-40e1-b410-a71e099d4ab9"},"outputs":[{"data":{"text/plain":["array([1, 1, 0, 0])"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["model.predict(prova)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOBwwAiS0aXMyUFSmZL0lRt","collapsed_sections":[],"name":"SVM.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
